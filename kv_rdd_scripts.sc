// first word as key
val lines = sc.textFile("README.md")
val pairs = lines.map(x => (x.split(" ")(0), x))

// filter
pairs.filter{case (key, value) => value.length < 20}
rdd.mapValues(x => (x, 1)).reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2))

// word count
val words = lines.flatMap(x => x.split(" "))
val result = words.map(x => (x, 1)).reduceByKey((x, y) => x + y)

// per-key average
val result = input.combineByKey(
(v) => (v, 1),
(acc: (Int, Int), v) => (acc._1 + v, acc._2 + 1),
(acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)
).map{ case (key, value) => (key, value._1 / value._2.toFloat) }
result.collectAsMap().map(println(_))

// tuning parallelism
val data = Seq(("a", 3), ("b", 4), ("a", 1))
sc.parallelize(data).reduceByKey((x, y) => x + y)
sc.parallelize(data).reduceByKey((x, y) => x + y, 10)

// partition information
val pairs = sc.parallelize(List((1, 1), (2, 2), (3, 3)))
pairs.partitioner
// add persist to use partition information in future operations
val partitioned = pairs.partitionBy(new spark.HashPartitioner(2))
partitioned.partitioner